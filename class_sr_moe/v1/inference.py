import argparse
import os
import torch
import torch.nn.functional as F
from PIL import Image
from torchvision import transforms

from fused_model import AdaptiveSRSystem


def inference_whole_image(
    image_path,
    model,
    device,
    patch_size=112,  # ËæìÂÖ•ÁΩëÁªúÁöÑÂàáÁâáÂ§ßÂ∞è
    overlap=16,  # ËæπÁºòÈáçÂè†Â§ßÂ∞è(Â∞ÜË¢´ÂàáÈô§ÁöÑËæπÁºòÂÆΩÂ∫¶)
    scale=2,
    output_path=None,
):

    # ËØªÂèñ‰∏éÂ§ÑÁêÜ
    img = Image.open(image_path).convert('RGB')
    w, h = img.size

    to_tensor = transforms.ToTensor()
    img_tensor = to_tensor(img).unsqueeze(0).to(device)  # (1, 3, H, W)

    # ÂÖ®Â±ÄÂàÜÁ±ªÔºàÁº©ÊîæÂà∞224*224Ôºâ
    classifier_input = F.interpolate(img_tensor, size=(224, 224), mode='bilinear', align_corners=False)

    with torch.no_grad():
        class_logits = model.classifier(classifier_input)
        scene_idx = torch.argmax(class_logits, dim=1).item()

    print(f"üîç Scene Analysis: Detected Class {scene_idx}")
    selected_expert = model.sr_experts[scene_idx]

    # ===== Êó†ÁºùÂàÜÂùóË∂ÖÂàÜ =====
    # ËÆ°ÁÆóÊ≠•Èïø(stride)ÔºåÂç≥ÊØèÊ¨°ÁßªÂä®ÁöÑË∑ùÁ¶ª
    stride = patch_size - (2 * overlap)

    # ÂØπÂéüÂõæËøõË°åÂèçÂ∞ÑÂ°´ÂÖÖ
    # ËæπÁºòÁöÑÂàáÁâá‰πüËÉΩËé∑Âæó‰∏ä‰∏ãÊñáÔºå‰∏î‰∏ç‰ºö‰∫ßÁîüÈªëËæπ
    img_padded = F.pad(img_tensor, (overlap, overlap, overlap, overlap), mode='reflect')
    pad_h, pad_w = img_padded.shape[2], img_padded.shape[3]

    # ÂàùÂßãÂåñËæìÂá∫ÁîªÂ∏É
    out_h, out_w = h * scale, w * scale
    output = torch.zeros((1, 3, out_h, out_w), device=device)

    # ÂèåÈáçÂæ™ÁéØÈÅçÂéÜ(Âú®Â°´ÂÖÖÂêéÁöÑÂõæÂÉè‰∏äÊªëÂä®)
    # y, x ÊòØÂú®‚ÄúÂéüÂßãÂõæÂÉè‚ÄùÂùêÊ†áÁ≥ª‰∏ãÁöÑËµ∑ÂßãÁÇπ
    for y in range(0, h, stride):
        for x in range(0, w, stride):

            # --- ÊèêÂèñËæìÂÖ•ÂàáÁâá ---
            # Ë¶ÜÁõñÂéüÂõæÁöÑ [y : y+stride] Âå∫Âüü
            # Âú® Padded Âõæ‰∏≠ÔºåËØ•Âå∫ÂüüÂØπÂ∫îÁöÑÂùêÊ†áÊòØ [y+overlap : y+stride+overlap]
            # ‰∏∫‰∫ÜËé∑Âæó‰∏ä‰∏ãÊñáÔºåÊàë‰ª¨ÂêëÂ§ñÊâ©Â±ï overlapÔºåÊâÄ‰ª•Âèñ [y : y+stride+2*overlap]
            # Âç≥Ôºö[y : y+patch_size]

            in_y_start = y
            in_x_start = x
            in_y_end = in_y_start + patch_size
            in_x_end = in_x_start + patch_size

            # ËæπÁïåÂ§ÑÁêÜÔºöÂ¶ÇÊûúË∂ÖÂá∫‰∫ÜÂ°´ÂÖÖÂêéÁöÑÂõæÂÉèËæπÁïåÔºåÂ∞±Âè™ÂèñÊúÄÂêéËÉΩÂèñÂà∞ÁöÑÈÉ®ÂàÜ
            if in_y_end > pad_h:
                in_y_start = pad_h - patch_size
                in_y_end = pad_h
            if in_x_end > pad_w:
                in_x_start = pad_w - patch_size
                in_x_end = pad_w

            in_patch = img_padded[:, :, in_y_start:in_y_end, in_x_start:in_x_end]

            # --- ‰∏ìÂÆ∂Êé®ÁêÜ ---
            with torch.no_grad():
                sr_patch = selected_expert(in_patch)

            # --- Ë£ÅÂâ™‰∏éÊãºÊé• ---
            # sr_patch ÁöÑÂ§ßÂ∞èÊòØ (patch_size * scale)
            # Âè™‰øùÁïô‰∏≠Èó¥ÁöÑÊúâÊïàÂå∫ÂüüÔºåÂàáÈô§ÂõõÂë®ÁöÑ overlap * scale

            # ËÆ°ÁÆóËæìÂá∫ÂàáÁâá‰∏≠‚ÄúÊúâÊïàÂå∫Âüü‚ÄùÁöÑËµ∑Ê≠¢ÁÇπ
            out_crop_start = overlap * scale
            out_crop_end = (patch_size - overlap) * scale

            # 1. Á°ÆÂÆö sr_patch ÂØπÂ∫îÁöÑÂéüÂõæËæìÂá∫ÂùêÊ†á
            # Âõ†‰∏∫ËæìÂÖ•ÊòØ img_padded[y_start...]ÔºåÂÆÉÂØπÂ∫îÁöÑÂéüÂõæÂùêÊ†áÊòØ (y_start - overlap)
            # ÊâÄ‰ª•ËæìÂá∫ÂØπÂ∫îÁöÑÂéüÂõæÂùêÊ†áÊòØ (y_start - overlap) * scale
            abs_y_start = (in_y_start - overlap) * scale
            abs_x_start = (in_x_start - overlap) * scale

            # 2. Ë£ÅÂâ™ÊéâËæπÁºò (ÂéªÈô§ artifacts)
            valid_sr = sr_patch[:, :, out_crop_start:out_crop_end, out_crop_start:out_crop_end]

            # 3. ËÆ°ÁÆóÁ≤òË¥¥Âà∞Â§ßÂõæÁöÑ‰ΩçÁΩÆ
            # ÊúâÊïàÂå∫ÂüüÂú®ÂéüÂõæ‰∏≠ÁöÑËµ∑Âßã‰ΩçÁΩÆ
            paste_y = abs_y_start + out_crop_start
            paste_x = abs_x_start + out_crop_start

            paste_h, paste_w = valid_sr.shape[2], valid_sr.shape[3]

            # 4. Á≤òË¥¥ (Ê≥®ÊÑèËæπÁïåÊ£ÄÊü•ÔºåÈò≤Ê≠¢Ê∫¢Âá∫)
            # Âè™ÊúâÂΩì paste_y >= 0 Êó∂ÊâçÁ≤òË¥¥
            y1 = max(0, paste_y)
            x1 = max(0, paste_x)
            y2 = min(out_h, paste_y + paste_h)
            x2 = min(out_w, paste_x + paste_w)

            # ÂØπÂ∫îÁöÑ valid_sr ÂÜÖÈÉ®ÂàáÁâá
            vy1 = y1 - paste_y
            vx1 = x1 - paste_x
            vy2 = vy1 + (y2 - y1)
            vx2 = vx1 + (x2 - x1)

            output[:, :, y1:y2, x1:x2] = valid_sr[:, :, vy1:vy2, vx1:vx2]

    # ‰øùÂ≠òÁªìÊûú
    to_pil = transforms.ToPILImage()
    result_img = to_pil(output.squeeze(0).cpu().clamp(0, 1))
    save_name = output_path or f"result_class_{scene_idx}.png"
    result_img.save(save_name)
    print(f"Done! Saved seamless result to {save_name} using Expert {scene_idx}")
    return save_name


def is_image_file(path: str) -> bool:
    ext = os.path.splitext(path)[1].lower()
    return ext in {".png", ".jpg", ".jpeg", ".bmp", ".tif", ".tiff", ".webp"}


def load_system_for_inference(sr_weights_path, classifier_weights_path, num_classes, sr_scale=2):
    # ÂàùÂßãÂåñÁ≥ªÁªü
    model = AdaptiveSRSystem(
        num_classes=num_classes,
        sr_scale=sr_scale,
        training_experts_only=False
    )

    # Âä†ËΩΩsrÊùÉÈáç
    print(f"Loading SR weights from {sr_weights_path}...")
    sr_state = torch.load(sr_weights_path, map_location='cpu')
    model.load_state_dict(sr_state, strict=False)

    # Âä†ËΩΩÂàÜÁ±ªÂô®ÊùÉÈáç
    print(f"Loading Classifier weights from {classifier_weights_path}...")
    cls_state = torch.load(classifier_weights_path, map_location='cpu')
    model.classifier.load_state_dict(cls_state)

    model.eval()
    return model


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Seamless tiled SR inference.")
    parser.add_argument("--image", default=None, help="Path to input image")
    parser.add_argument("--input-dir", default="test_input", help="Folder containing input images")
    parser.add_argument("--output-dir", default="test_output", help="Folder to save outputs for batch mode")
    parser.add_argument("--sr-weights", required=True, help="Path to SR (fused) weights")
    parser.add_argument("--class-weights", required=True, help="Path to classifier weights")
    parser.add_argument("--num-classes", type=int, default=3, help="Number of classes")
    parser.add_argument("--scale", type=int, default=2, help="SR scale factor")
    parser.add_argument("--device", default="cuda" if torch.cuda.is_available() else "cpu")
    parser.add_argument("--patch-size", type=int, default=112, help="Patch size with context")
    parser.add_argument("--overlap", type=int, default=16, help="Overlap size to crop")
    parser.add_argument("--output", default=None, help="Output image path")
    args = parser.parse_args()

    fused_model = load_system_for_inference(
        sr_weights_path=args.sr_weights,
        classifier_weights_path=args.class_weights,
        num_classes=args.num_classes,
        sr_scale=args.scale,
    )
    fused_model = fused_model.to(args.device)

    if args.input_dir:
        if not os.path.isdir(args.input_dir):
            raise FileNotFoundError(f"Input dir not found: {args.input_dir}")
        output_dir = args.output_dir or os.path.join(args.input_dir, "sr_results")
        os.makedirs(output_dir, exist_ok=True)

        filenames = sorted(os.listdir(args.input_dir))
        image_paths = [
            os.path.join(args.input_dir, name)
            for name in filenames
            if is_image_file(name)
        ]
        if not image_paths:
            raise FileNotFoundError(f"No images found in: {args.input_dir}")

        for image_path in image_paths:
            base = os.path.splitext(os.path.basename(image_path))[0]
            output_path = os.path.join(output_dir, f"{base}_sr.png")
            inference_whole_image(
                image_path,
                fused_model,
                args.device,
                patch_size=args.patch_size,
                overlap=args.overlap,
                scale=args.scale,
                output_path=output_path,
            )
    elif args.image:
        inference_whole_image(
            args.image,
            fused_model,
            args.device,
            patch_size=args.patch_size,
            overlap=args.overlap,
            scale=args.scale,
            output_path=args.output,
        )
    else:
        raise ValueError("Provide --image or --input-dir for inference.")
